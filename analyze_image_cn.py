#!/usr/bin/env python3
"""
BLIP2 å›¾åƒåˆ†æè„šæœ¬ - ä¸­æ–‡ç‰ˆï¼Œä¸“æ³¨äºæ„å›¾å’Œè‰²è°ƒåˆ†æ
"""
import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import argparse
import os

def setup_device():
    """è®¾ç½®è¿è¡Œè®¾å¤‡"""
    if torch.cuda.is_available():
        device = "cuda"
        print("ä½¿ç”¨ NVIDIA GPU (CUDA)")
    elif torch.backends.mps.is_available():
        device = "cpu"
        print("ä½¿ç”¨ CPU (MPS å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜)")
    else:
        device = "cpu"
        print("ä½¿ç”¨ CPU")
    return device

def load_model(device):
    """åŠ è½½BLIP2æ¨¡å‹"""
    print("\næ­£åœ¨åŠ è½½ BLIP2 æ¨¡å‹...")
    
    processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
    
    if device == "cuda":
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        ).to(device)
    else:
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            low_cpu_mem_usage=True
        ).to(device)
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return processor, model

def load_image(image_path):
    """åŠ è½½æœ¬åœ°å›¾ç‰‡"""
    if os.path.exists(image_path):
        print(f"ä»æœ¬åœ°åŠ è½½å›¾ç‰‡: {image_path}")
        image = Image.open(image_path).convert('RGB')
        width, height = image.size
        print(f"å›¾ç‰‡å°ºå¯¸: {width}x{height}")
    else:
        raise ValueError(f"æ— æ³•æ‰¾åˆ°å›¾ç‰‡: {image_path}")
    return image

def analyze_with_prompt(processor, model, image, prompt, device, max_tokens=60):
    """ä½¿ç”¨ç‰¹å®šæç¤ºè¯åˆ†æå›¾ç‰‡"""
    inputs = processor(image, prompt, return_tensors="pt").to(device)
    
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        num_beams=5,
        do_sample=True,
        top_p=0.95
    )
    
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    if "Answer:" in generated_text:
        answer = generated_text.split("Answer:")[-1].strip()
    else:
        answer = generated_text.replace(prompt, "").strip()
    
    return answer

def comprehensive_analysis(processor, model, image, device):
    """ç»¼åˆåˆ†æå›¾ç‰‡çš„æ„å›¾å’Œè‰²è°ƒ"""
    
    print("\nå¼€å§‹åˆ†æå›¾ç‰‡...")
    results = {}
    
    # åŸºç¡€æè¿°
    base_prompt = "This is a photograph showing"
    base_inputs = processor(image, base_prompt, return_tensors="pt").to(device)
    base_ids = model.generate(**base_inputs, max_new_tokens=80, num_beams=5, do_sample=True, top_p=0.9)
    base_desc = processor.batch_decode(base_ids, skip_special_tokens=True)[0].replace(base_prompt, "").strip()
    results['åŸºç¡€æè¿°'] = base_desc
    
    # æ„å›¾åˆ†æ - ä½¿ç”¨æ›´å…·ä½“çš„æç¤ºè¯
    composition_prompts = {
        'æ„å›¾ç±»å‹': "Question: Is this image using rule of thirds, centered composition, symmetrical, diagonal lines, or leading lines? Answer:",
        'å‰æ™¯ä¸­æ™¯èƒŒæ™¯': "Question: Describe the foreground, middle ground, and background elements. Answer:",
        'è§†è§‰å¹³è¡¡': "Question: Is the visual weight balanced or asymmetrical in this photo? Answer:",
        'æ°´å¹³çº¿ä½ç½®': "Question: Where is the horizon line positioned in the frame? Answer:",
        'è§†è§‰ç„¦ç‚¹': "Question: What is the main subject or focal point? Answer:"
    }
    
    for key, prompt in composition_prompts.items():
        answer = analyze_with_prompt(processor, model, image, prompt, device, max_tokens=40)
        results[key] = answer
    
    # è‰²è°ƒåˆ†æ - ä½¿ç”¨æ›´å…·ä½“çš„æç¤ºè¯  
    color_prompts = {
        'ä¸»è¦é¢œè‰²': "Question: What are the main colors visible? List the top 3 colors. Answer:",
        'è‰²å½©æ°›å›´': "Question: Are the colors warm (reds/oranges/yellows) or cool (blues/greens/purples)? Answer:",
        'å…‰çº¿ç‰¹å¾': "Question: What type of lighting is present - sunrise, sunset, midday, overcast, or golden hour? Answer:",
        'è‰²å½©å¯¹æ¯”': "Question: Is there high contrast or low contrast between colors? Answer:",
        'å¤©ç©ºç‰¹å¾': "Question: If there's a sky, describe its colors and cloud patterns. Answer:"
    }
    
    for key, prompt in color_prompts.items():
        answer = analyze_with_prompt(processor, model, image, prompt, device, max_tokens=40)
        results[key] = answer
    
    return results

def interpret_results(results):
    """å°†è‹±æ–‡ç»“æœè§£é‡Šä¸ºä¸­æ–‡åˆ†æ"""
    print("\n" + "="*80)
    print("ğŸ“¸ å›¾åƒä¸“ä¸šåˆ†ææŠ¥å‘Š")
    print("="*80)
    
    print("\nã€ğŸ“ åœºæ™¯æ¦‚è¿°ã€‘")
    print("-"*60)
    base_desc = results.get('åŸºç¡€æè¿°', 'N/A')
    print(f"åœºæ™¯å†…å®¹: {base_desc}")
    
    print("\nã€ğŸ¨ æ„å›¾åˆ†æã€‘")
    print("-"*60)
    
    # æ„å›¾ç±»å‹åˆ¤æ–­
    comp_type = results.get('æ„å›¾ç±»å‹', '').lower()
    if 'rule of thirds' in comp_type:
        print("â€¢ æ„å›¾æ³•åˆ™: ä¸‰åˆ†æ³•æ„å›¾")
    elif 'centered' in comp_type or 'center' in comp_type:
        print("â€¢ æ„å›¾æ³•åˆ™: ä¸­å¿ƒæ„å›¾")
    elif 'symmetr' in comp_type:
        print("â€¢ æ„å›¾æ³•åˆ™: å¯¹ç§°æ„å›¾")
    elif 'diagonal' in comp_type:
        print("â€¢ æ„å›¾æ³•åˆ™: å¯¹è§’çº¿æ„å›¾")
    elif 'leading lines' in comp_type:
        print("â€¢ æ„å›¾æ³•åˆ™: å¼•å¯¼çº¿æ„å›¾")
    else:
        print(f"â€¢ æ„å›¾ç‰¹å¾: {comp_type}")
    
    # ç©ºé—´å±‚æ¬¡
    layers = results.get('å‰æ™¯ä¸­æ™¯èƒŒæ™¯', '')
    if layers:
        print(f"â€¢ ç©ºé—´å±‚æ¬¡: {layers}")
    
    # è§†è§‰å¹³è¡¡
    balance = results.get('è§†è§‰å¹³è¡¡', '').lower()
    if 'balanced' in balance and 'asymmetrical' not in balance:
        print("â€¢ è§†è§‰å¹³è¡¡: å¹³è¡¡æ„å›¾ï¼Œå·¦å³è§†è§‰é‡é‡ç›¸å½“")
    elif 'asymmetrical' in balance:
        print("â€¢ è§†è§‰å¹³è¡¡: éå¯¹ç§°æ„å›¾ï¼Œåˆ›é€ åŠ¨æ„Ÿ")
    else:
        print(f"â€¢ è§†è§‰å¹³è¡¡: {balance}")
    
    # åœ°å¹³çº¿
    horizon = results.get('æ°´å¹³çº¿ä½ç½®', '').lower()
    if 'top' in horizon or 'upper' in horizon:
        print("â€¢ åœ°å¹³çº¿: ä½äºç”»é¢ä¸Šä¸‰åˆ†ä¹‹ä¸€ï¼Œå¼ºè°ƒå‰æ™¯")
    elif 'bottom' in horizon or 'lower' in horizon:
        print("â€¢ åœ°å¹³çº¿: ä½äºç”»é¢ä¸‹ä¸‰åˆ†ä¹‹ä¸€ï¼Œå¼ºè°ƒå¤©ç©º")
    elif 'middle' in horizon or 'center' in horizon:
        print("â€¢ åœ°å¹³çº¿: ä½äºç”»é¢ä¸­å¤®")
    else:
        print(f"â€¢ åœ°å¹³çº¿ä½ç½®: {horizon}")
    
    # ç„¦ç‚¹
    print(f"â€¢ è§†è§‰ç„¦ç‚¹: {results.get('è§†è§‰ç„¦ç‚¹', 'N/A')}")
    
    print("\nã€ğŸŒˆ è‰²è°ƒåˆ†æã€‘")
    print("-"*60)
    
    # ä¸»è‰²è°ƒ
    colors = results.get('ä¸»è¦é¢œè‰²', '')
    print(f"â€¢ ä¸»è¦è‰²å½©: {colors}")
    
    # è‰²æ¸©
    temp = results.get('è‰²å½©æ°›å›´', '').lower()
    if 'warm' in temp:
        print("â€¢ è‰²æ¸©: æš–è‰²è°ƒ (åçº¢æ©™é»„)ï¼Œè¥é€ æ¸©æš–ã€æ´»åŠ›çš„æ°›å›´")
    elif 'cool' in temp:
        print("â€¢ è‰²æ¸©: å†·è‰²è°ƒ (åè“ç»¿ç´«)ï¼Œè¥é€ å®é™ã€æ¸…çˆ½çš„æ°›å›´")
    else:
        print(f"â€¢ è‰²æ¸©ç‰¹å¾: {temp}")
    
    # å…‰çº¿
    light = results.get('å…‰çº¿ç‰¹å¾', '').lower()
    if 'sunrise' in light:
        print("â€¢ å…‰çº¿æ—¶åˆ»: æ—¥å‡ºæ—¶åˆ†ï¼ŒæŸ”å’Œçš„æ™¨å…‰")
    elif 'sunset' in light:
        print("â€¢ å…‰çº¿æ—¶åˆ»: æ—¥è½æ—¶åˆ†ï¼Œæ¸©æš–çš„å¤•é˜³å…‰")
    elif 'golden hour' in light:
        print("â€¢ å…‰çº¿æ—¶åˆ»: é»„é‡‘æ—¶åˆ»ï¼Œæœ€ä½³æ‹æ‘„å…‰çº¿")
    elif 'midday' in light:
        print("â€¢ å…‰çº¿æ—¶åˆ»: æ­£åˆï¼Œå¼ºçƒˆçš„æ—¥å…‰")
    elif 'overcast' in light:
        print("â€¢ å…‰çº¿æ—¶åˆ»: é˜´å¤©ï¼ŒæŸ”å’Œå‡åŒ€çš„æ•£å°„å…‰")
    else:
        print(f"â€¢ å…‰çº¿ç‰¹å¾: {light}")
    
    # å¯¹æ¯”åº¦
    contrast = results.get('è‰²å½©å¯¹æ¯”', '').lower()
    if 'high contrast' in contrast:
        print("â€¢ å¯¹æ¯”åº¦: é«˜å¯¹æ¯”åº¦ï¼Œç”»é¢å±‚æ¬¡åˆ†æ˜")
    elif 'low contrast' in contrast:
        print("â€¢ å¯¹æ¯”åº¦: ä½å¯¹æ¯”åº¦ï¼Œç”»é¢æŸ”å’Œç»Ÿä¸€")
    else:
        print(f"â€¢ å¯¹æ¯”ç‰¹å¾: {contrast}")
    
    # å¤©ç©º
    sky = results.get('å¤©ç©ºç‰¹å¾', '')
    if sky and sky != 'N/A':
        print(f"â€¢ å¤©ç©ºæè¿°: {sky}")
    
    print("\nã€ğŸ’¡ ä¸“ä¸šå»ºè®®ã€‘")
    print("-"*60)
    
    # åŸºäºåˆ†æç»™å‡ºå»ºè®®
    if 'sunset' in light or 'sunrise' in light:
        print("â€¢ è¿™æ˜¯é»„é‡‘æ—¶åˆ»æ‹æ‘„ï¼Œå…‰çº¿æ¡ä»¶æä½³")
    
    if 'rule of thirds' in comp_type:
        print("â€¢ æ„å›¾éµå¾ªä¸‰åˆ†æ³•ï¼Œç”»é¢å¹³è¡¡æ„Ÿè‰¯å¥½")
    
    if 'warm' in temp and ('sunset' in light or 'sunrise' in light):
        print("â€¢ æš–è‰²è°ƒä¸é»„é‡‘æ—¶åˆ»å®Œç¾é…åˆï¼Œè¥é€ æ¸©é¦¨æ°›å›´")
    
    if 'reflection' in base_desc.lower() or 'water' in base_desc.lower():
        print("â€¢ æ°´é¢å€’å½±å¢åŠ äº†ç”»é¢çš„å¯¹ç§°ç¾å’Œç©ºé—´æ·±åº¦")

def main():
    parser = argparse.ArgumentParser(description='BLIP2 å›¾åƒæ„å›¾ä¸è‰²è°ƒä¸“ä¸šåˆ†æ')
    parser.add_argument(
        '--image', 
        type=str, 
        default='MinnesotaWaters.jpg',
        help='å›¾ç‰‡è·¯å¾„'
    )
    
    args = parser.parse_args()
    
    device = setup_device()
    processor, model = load_model(device)
    image = load_image(args.image)
    
    # æ‰§è¡Œç»¼åˆåˆ†æ
    results = comprehensive_analysis(processor, model, image, device)
    
    # è§£é‡Šç»“æœ
    interpret_results(results)
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("\næç¤º: BLIP2 æ¨¡å‹çš„åˆ†æåŸºäºè§†è§‰ç†è§£ï¼Œå¯èƒ½å­˜åœ¨ä¸€å®šåå·®ã€‚")
    print("å»ºè®®ç»“åˆäººå·¥åˆ¤æ–­ä»¥è·å¾—æœ€å‡†ç¡®çš„å›¾åƒåˆ†æã€‚")

if __name__ == "__main__":
    main()