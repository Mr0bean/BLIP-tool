#!/usr/bin/env python3
"""
BLIP2 æ¨¡å‹è¿è¡Œè„šæœ¬
æ”¯æŒæœ¬åœ°å›¾ç‰‡å’ŒURLå›¾ç‰‡çš„æè¿°ç”Ÿæˆå’Œè§†è§‰é—®ç­”
"""
import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import requests
import argparse
import os

def setup_device():
    """è®¾ç½®è¿è¡Œè®¾å¤‡"""
    if torch.cuda.is_available():
        device = "cuda"
        print("ä½¿ç”¨ NVIDIA GPU (CUDA)")
    elif torch.backends.mps.is_available():
        # MPS å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œå¯é€‰æ‹©ä½¿ç”¨
        device = "cpu"  # æ”¹ä¸º "mps" å¦‚æœæƒ³å°è¯•ä½¿ç”¨ Apple Silicon GPU
        print("ä½¿ç”¨ CPU (MPS å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜)")
    else:
        device = "cpu"
        print("ä½¿ç”¨ CPU")
    return device

def load_model(device):
    """åŠ è½½BLIP2æ¨¡å‹"""
    print("\næ­£åœ¨åŠ è½½ BLIP2 æ¨¡å‹...")
    
    processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
    
    if device == "cuda":
        # GPUä¸Šå¯ä»¥ä½¿ç”¨åŠç²¾åº¦
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        ).to(device)
    else:
        # CPUä¸Šä½¿ç”¨å…¨ç²¾åº¦
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            low_cpu_mem_usage=True
        ).to(device)
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return processor, model

def load_image(image_source):
    """åŠ è½½å›¾ç‰‡ï¼ˆæ”¯æŒURLå’Œæœ¬åœ°æ–‡ä»¶ï¼‰"""
    if image_source.startswith('http'):
        print(f"ä»URLåŠ è½½å›¾ç‰‡: {image_source}")
        image = Image.open(requests.get(image_source, stream=True).raw).convert('RGB')
    elif os.path.exists(image_source):
        print(f"ä»æœ¬åœ°åŠ è½½å›¾ç‰‡: {image_source}")
        image = Image.open(image_source).convert('RGB')
    else:
        raise ValueError(f"æ— æ³•åŠ è½½å›¾ç‰‡: {image_source}")
    return image

def generate_caption(processor, model, image, device):
    """ç”Ÿæˆå›¾ç‰‡æè¿°"""
    print("\nç”Ÿæˆå›¾ç‰‡æè¿°...")
    inputs = processor(image, return_tensors="pt").to(device)
    
    # ä½¿ç”¨é‡‡æ ·å‚æ•°æ¥è·å¾—æ›´å¤šæ ·çš„è¾“å‡º
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=50,
        num_beams=5,
        temperature=0.8
    )
    
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    return caption

def answer_question(processor, model, image, question, device):
    """å›ç­”å…³äºå›¾ç‰‡çš„é—®é¢˜"""
    # ä¸ºé—®é¢˜æ·»åŠ å‰ç¼€ï¼Œè¿™å¯¹BLIP2å¾ˆé‡è¦
    prompt = f"Question: {question} Answer:"
    
    inputs = processor(image, prompt, return_tensors="pt").to(device)
    
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=30,
        num_beams=3,
        temperature=0.7
    )
    
    # åªè·å–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    # æå–ç­”æ¡ˆéƒ¨åˆ†
    if "Answer:" in generated_text:
        answer = generated_text.split("Answer:")[-1].strip()
    else:
        answer = generated_text.strip()
    
    return answer

def main():
    parser = argparse.ArgumentParser(description='è¿è¡Œ BLIP2 æ¨¡å‹')
    parser.add_argument(
        '--image', 
        type=str, 
        default='https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg',
        help='å›¾ç‰‡è·¯å¾„æˆ–URL'
    )
    parser.add_argument(
        '--mode',
        type=str,
        choices=['caption', 'vqa', 'both'],
        default='both',
        help='è¿è¡Œæ¨¡å¼ï¼šcaption(æè¿°), vqa(é—®ç­”), both(ä¸¤è€…éƒ½è¿è¡Œ)'
    )
    parser.add_argument(
        '--question',
        type=str,
        default=None,
        help='è¦é—®çš„é—®é¢˜ï¼ˆVQAæ¨¡å¼ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device()
    
    # åŠ è½½æ¨¡å‹
    processor, model = load_model(device)
    
    # åŠ è½½å›¾ç‰‡
    image = load_image(args.image)
    
    print("\n" + "="*60)
    
    # # ç”Ÿæˆæè¿°
    # if args.mode in ['caption', 'both']:
    #     caption = generate_caption(processor, model, image, device)
    #     print(f"ğŸ“ å›¾ç‰‡æè¿°: {caption}")
    #     print("="*60)
    
    # è§†è§‰é—®ç­”
    if args.mode in ['vqa', 'both']:
        if args.question:
            # ä½¿ç”¨ç”¨æˆ·æä¾›çš„é—®é¢˜
            answer = answer_question(processor, model, image, args.question, device)
            print(f"â“ é—®é¢˜: {args.question}")
            print(f"ğŸ’¬ å›ç­”: {answer}")
        else:
            # é»˜è®¤é—®é¢˜é›†
            print("\nè§†è§‰é—®ç­”æ¼”ç¤º:")
            print("-"*40)
            
            # demo_questions = [
            #     "What is in this image?",
            #     "How many people are there?",
            #     "What is the main color?",
            #     "What is happening in this picture?"
            # ]
            
            
            demo_questions = [
                # "what is the composition of the image?"
                "what is the composition of the image {Rule of Thirds} or {Symmetrical Composition} or {Diagonal Composition} or {Leading Lines Composition} or {Framing Composition} or {Central Composition} or {Golden Ratio / Golden Spiral} or {Negative Space Composition} or {Repetition & Pattern Composition} or {Asymmetrical Balance} or {Radial Composition} or {Layering Composition}"
            ]

            for q in demo_questions:
                answer = answer_question(processor, model, image, q, device)
                print(f"\nâ“ é—®é¢˜: {q}")
                print(f"ğŸ’¬ å›ç­”: {answer}")
        
        print("\n" + "="*60)
    
    print("\nâœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()