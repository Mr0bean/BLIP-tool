#!/usr/bin/env python3
"""
BLIP2 å›¾åƒåˆ†æè„šæœ¬ - ä¸“æ³¨äºæ„å›¾å’Œè‰²è°ƒåˆ†æ
"""
import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import argparse
import os

def setup_device():
    """è®¾ç½®è¿è¡Œè®¾å¤‡"""
    if torch.cuda.is_available():
        device = "cuda"
        print("ä½¿ç”¨ NVIDIA GPU (CUDA)")
    elif torch.backends.mps.is_available():
        device = "cpu"
        print("ä½¿ç”¨ CPU (MPS å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜)")
    else:
        device = "cpu"
        print("ä½¿ç”¨ CPU")
    return device

def load_model(device):
    """åŠ è½½BLIP2æ¨¡å‹"""
    print("\næ­£åœ¨åŠ è½½ BLIP2 æ¨¡å‹...")
    
    processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
    
    if device == "cuda":
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        ).to(device)
    else:
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            low_cpu_mem_usage=True
        ).to(device)
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return processor, model

def load_image(image_path):
    """åŠ è½½æœ¬åœ°å›¾ç‰‡"""
    if os.path.exists(image_path):
        print(f"ä»æœ¬åœ°åŠ è½½å›¾ç‰‡: {image_path}")
        image = Image.open(image_path).convert('RGB')
    else:
        raise ValueError(f"æ— æ³•æ‰¾åˆ°å›¾ç‰‡: {image_path}")
    return image

def generate_caption(processor, model, image, device):
    """ç”Ÿæˆå›¾ç‰‡çš„æ€»ä½“æè¿°"""
    inputs = processor(image, return_tensors="pt").to(device)
    
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=100,
        num_beams=5,
        temperature=0.8,
        do_sample=True
    )
    
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    return caption

def analyze_with_prompt(processor, model, image, prompt, device, max_tokens=50):
    """ä½¿ç”¨ç‰¹å®šæç¤ºè¯åˆ†æå›¾ç‰‡"""
    inputs = processor(image, prompt, return_tensors="pt").to(device)
    
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        num_beams=4,
        temperature=0.7,
        do_sample=True
    )
    
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    if "Answer:" in generated_text:
        answer = generated_text.split("Answer:")[-1].strip()
    else:
        answer = generated_text.strip()
    
    return answer

def analyze_composition(processor, model, image, device):
    """åˆ†æå›¾ç‰‡æ„å›¾"""
    composition_prompts = [
        "Question: Describe the composition of this image in terms of visual arrangement and balance. Answer:",
        "Question: What compositional technique is used in this photograph? Answer:",
        "Question: How are the elements arranged in this image? Answer:",
        "Question: Describe the rule of thirds, symmetry, or leading lines in this image. Answer:",
        "Question: What is the focal point and how does the composition guide the viewer's eye? Answer:"
    ]
    
    results = []
    for prompt in composition_prompts:
        answer = analyze_with_prompt(processor, model, image, prompt, device, max_tokens=80)
        results.append(answer)
    
    return results

def analyze_color_tone(processor, model, image, device):
    """åˆ†æå›¾ç‰‡è‰²è°ƒ"""
    color_prompts = [
        "Question: What are the dominant colors and color palette in this image? Answer:",
        "Question: Describe the mood created by the color tones (warm, cool, neutral)? Answer:",
        "Question: What time of day does the lighting and color suggest? Answer:",
        "Question: How would you describe the color harmony and contrast in this image? Answer:",
        "Question: What emotions do the colors in this image evoke? Answer:"
    ]
    
    results = []
    for prompt in color_prompts:
        answer = analyze_with_prompt(processor, model, image, prompt, device, max_tokens=60)
        results.append(answer)
    
    return results

def main():
    parser = argparse.ArgumentParser(description='BLIP2 å›¾åƒæ„å›¾ä¸è‰²è°ƒåˆ†æ')
    parser.add_argument(
        '--image', 
        type=str, 
        default='MinnesotaWaters.jpg',
        help='å›¾ç‰‡è·¯å¾„'
    )
    
    args = parser.parse_args()
    
    device = setup_device()
    processor, model = load_model(device)
    image = load_image(args.image)
    
    print("\n" + "="*80)
    print("ğŸ“¸ å›¾åƒåˆ†ææŠ¥å‘Š")
    print("="*80)
    
    print("\nğŸ“ ã€æ€»ä½“æè¿°ã€‘")
    print("-"*60)
    caption = generate_caption(processor, model, image, device)
    print(f"å›¾åƒå†…å®¹: {caption}")
    
    print("\nğŸ¨ ã€æ„å›¾åˆ†æã€‘")
    print("-"*60)
    composition_analysis = analyze_composition(processor, model, image, device)
    composition_labels = [
        "è§†è§‰å¸ƒå±€",
        "æ„å›¾æŠ€å·§",
        "å…ƒç´ æ’åˆ—",
        "æ„å›¾è§„åˆ™",
        "è§†è§‰å¼•å¯¼"
    ]
    
    for label, analysis in zip(composition_labels, composition_analysis):
        print(f"â€¢ {label}: {analysis}")
    
    print("\nğŸŒˆ ã€è‰²è°ƒåˆ†æã€‘")
    print("-"*60)
    color_analysis = analyze_color_tone(processor, model, image, device)
    color_labels = [
        "ä¸»è‰²è°ƒ",
        "è‰²æ¸©æ°›å›´",
        "æ—¶é—´æ„ŸçŸ¥",
        "è‰²å½©å’Œè°",
        "æƒ…æ„Ÿè‰²å½©"
    ]
    
    for label, analysis in zip(color_labels, color_analysis):
        print(f"â€¢ {label}: {analysis}")
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()